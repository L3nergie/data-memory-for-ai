# Projet de Recherche S√©mantique en Temps R√©el

Ce projet est une application Python qui permet d'effectuer des recherches s√©mantiques en temps r√©el. Il offre une interface interactive pour ajouter des mots ou des phrases √† une base de donn√©es, effectuer des requ√™tes dynamiques, et ajuster les r√©sultats en fonction de la pertinence et de la longueur des phrases.

---

## Structure du Projet

```
projet/
‚îÇ
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ words.json          # Table des mots avec leurs hashs et relations
‚îÇ   ‚îú‚îÄ‚îÄ phrases.json        # Table des phrases avec leurs tokens et relations
‚îÇ
‚îú‚îÄ‚îÄ system/
‚îÇ   ‚îú‚îÄ‚îÄ query_processor.py  # Syst√®me de traitement des requ√™tes
‚îÇ   ‚îú‚îÄ‚îÄ data_handler.py     # Gestion des mots et phrases (ajout, v√©rification, hash)
‚îÇ
‚îú‚îÄ‚îÄ interface/
‚îÇ   ‚îú‚îÄ‚îÄ main.py             # Interface principale pour les requ√™tes en temps r√©el
‚îÇ   ‚îú‚îÄ‚îÄ add_data.py         # Widget pour ajouter des mots ou des phrases
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ hash_generator.py   # G√©n√©ration de hashs pour les mots
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py        # Tokenisation des phrases et v√©rification des limites
‚îÇ
‚îú‚îÄ‚îÄ README.md               # Documentation du projet
‚îú‚îÄ‚îÄ requirements.txt        # D√©pendances du projet
```

---

## Fonctionnalit√©s

1. **Recherche en Temps R√©el** :
   - Affichage des r√©sultats d√®s qu'un mot est entr√©.
   - Ajustement dynamique des r√©sultats en fonction des mots suppl√©mentaires.
   - Barre d'ajustement pour modifier le nombre de r√©sultats et la pertinence.

2. **Gestion des Mots et Phrases** :
   - V√©rification de l'existence des mots dans la base de donn√©es.
   - G√©n√©ration de hashs pour s√©curiser les mots.
   - Ajout de relations entre les mots et les phrases.
   - D√©coupage des phrases en tokens (entre 75 et 125 tokens) en respectant les points de fin de phrase.

3. **Interface Utilisateur** :
   - Champ de recherche interactif pour les requ√™tes.
   - Widget pour ajouter des mots ou des phrases √† la base de donn√©es.

4. **S√©curit√©** :
   - Les mots sont stock√©s sous forme de hashs, avec une cl√© de cryptage bas√©e sur le nombre de lettres.
   - Impossible d'avoir des doublons dans la table des mots.

---

## Utilisation

### Pr√©requis

- Python 3.x
- Biblioth√®ques : `json`, `hashlib`, `re`

### Installation

1. Clonez le d√©p√¥t du projet :

   ```bash
   git clone https://github.com/votre-utilisateur/votre-projet.git
   cd votre-projet
   ```

2. Installez les d√©pendances (si n√©cessaire) :

   ```bash
   pip install -r requirements.txt
   ```

---

## Fichiers et Fonctionnalit√©s D√©tails

### 1. `system/query_processor.py`

Ce fichier contient la logique de traitement des requ√™tes. Il g√®re la recherche en temps r√©el et l'ajustement des r√©sultats.

```python
import json
from typing import List, Dict, Tuple
from utils.hash_generator import generate_hash
from utils.tokenizer import tokenize_phrase

class QueryProcessor:
    def __init__(self, words_file: str, phrases_file: str):
        self.words_file = words_file
        self.phrases_file = phrases_file
        self.words_data = self._load_data(words_file)
        self.phrases_data = self._load_data(phrases_file)

    def _load_data(self, filename: str) -> Dict:
        try:
            with open(filename, "r") as file:
                return json.load(file)
        except FileNotFoundError:
            return {}

    def search(self, query: str, relevance_threshold: int = 50, max_results: int = 10) -> List[Tuple[str, float]]:
        results = []
        query_words = query.split()
        for phrase, metadata in self.phrases_data.items():
            relevance = self._calculate_relevance(query_words, metadata["tokens"])
            if relevance >= relevance_threshold:
                results.append((phrase, relevance))
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:max_results]

    def _calculate_relevance(self, query_words: List[str], phrase_tokens: List[str]) -> float:
        common_tokens = set(query_words).intersection(set(phrase_tokens))
        return (len(common_tokens) / len(query_words)) * 100
```

---

### 2. `system/data_handler.py`

Ce fichier g√®re l'ajout de mots et de phrases √† la base de donn√©es, ainsi que la v√©rification des doublons.

```python
import json
from utils.hash_generator import generate_hash
from utils.tokenizer import tokenize_phrase

class DataHandler:
    def __init__(self, words_file: str, phrases_file: str):
        self.words_file = words_file
        self.phrases_file = phrases_file
        self.words_data = self._load_data(words_file)
        self.phrases_data = self._load_data(phrases_file)

    def _load_data(self, filename: str) -> Dict:
        try:
            with open(filename, "r") as file:
                return json.load(file)
        except FileNotFoundError:
            return {}

    def add_word(self, word: str):
        word_hash = generate_hash(word)
        if word_hash not in self.words_data:
            self.words_data[word_hash] = {"word": word, "relations": []}
            self._save_data(self.words_file, self.words_data)

    def add_phrase(self, phrase: str):
        tokens = tokenize_phrase(phrase)
        if phrase not in self.phrases_data:
            self.phrases_data[phrase] = {"tokens": tokens, "relations": []}
            self._save_data(self.phrases_file, self.phrases_data)

    def _save_data(self, filename: str, data: Dict):
        with open(filename, "w") as file:
            json.dump(data, file, indent=4)
```

---

### 3. `utils/hash_generator.py`

Ce fichier g√©n√®re des hashs pour les mots en utilisant le nombre de lettres comme cl√© de cryptage.

```python
import hashlib

def generate_hash(word: str) -> str:
    key = str(len(word))
    return hashlib.sha256((word + key).encode()).hexdigest()
```

---

### 4. `utils/tokenizer.py`

Ce fichier tokenise les phrases et v√©rifie les limites de tokens (entre 75 et 125).

```python
import re

def tokenize_phrase(phrase: str) -> List[str]:
    # D√©coupage en tokens en respectant les points de fin de phrase
    tokens = re.split(r'[ !?.;"\']+', phrase)
    tokens = [token for token in tokens if token]
    return tokens
```

---

### 5. `interface/main.py`

Ce fichier contient l'interface principale pour les requ√™tes en temps r√©el.

```python
from system.query_processor import QueryProcessor

def main():
    query_processor = QueryProcessor("database/words.json", "database/phrases.json")
    while True:
        query = input("Entrez votre recherche : ")
        results = query_processor.search(query)
        for result, relevance in results:
            print(f"{result} (Pertinence : {relevance:.2f}%)")

if __name__ == "__main__":
    main()
```

---

### 6. `interface/add_data.py`

Ce fichier contient le widget pour ajouter des mots ou des phrases.

```python
from system.data_handler import DataHandler

def add_data():
    data_handler = DataHandler("database/words.json", "database/phrases.json")
    while True:
        data = input("Entrez un mot ou une phrase : ")
        if " " in data:
            data_handler.add_phrase(data)
        else:
            data_handler.add_word(data)
        print("Donn√©e ajout√©e avec succ√®s !")

if __name__ == "__main__":
    add_data()
```

---

## Conclusion

Ce projet est con√ßu pour √™tre modulaire et extensible. Vous pouvez facilement ajouter de nouvelles fonctionnalit√©s ou am√©liorer les existantes. N'h√©sitez pas √† adapter le code √† vos besoins sp√©cifiques !

---

N'h√©sitez pas √† me demander si vous avez besoin d'autres ajustements ou explications ! üòä
